{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66330c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "The following additional packages will be installed:\n",
      "  freeglut3 libegl-dev libgl-dev libgl1-mesa-dev libgles-dev libgles1\n",
      "  libglu1-mesa libglu1-mesa-dev libglvnd-core-dev libglvnd-dev libglx-dev\n",
      "  libopengl-dev libxt-dev\n",
      "Suggested packages:\n",
      "  libxt-doc\n",
      "The following NEW packages will be installed:\n",
      "  freeglut3 freeglut3-dev libegl-dev libgl-dev libgl1-mesa-dev libgles-dev\n",
      "  libgles1 libglu1-mesa libglu1-mesa-dev libglvnd-core-dev libglvnd-dev\n",
      "  libglx-dev libopengl-dev libxt-dev\n",
      "0 upgraded, 14 newly installed, 0 to remove and 41 not upgraded.\n",
      "Need to get 1,192 kB of archives.\n",
      "After this operation, 6,439 kB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 freeglut3 amd64 2.8.1-6 [74.0 kB]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglx-dev amd64 1.4.0-1 [14.1 kB]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgl-dev amd64 1.4.0-1 [101 kB]\n",
      "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglvnd-core-dev amd64 1.4.0-1 [12.7 kB]\n",
      "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 libegl-dev amd64 1.4.0-1 [18.0 kB]\n",
      "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgles1 amd64 1.4.0-1 [11.5 kB]\n",
      "Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgles-dev amd64 1.4.0-1 [49.4 kB]\n",
      "Get:8 http://archive.ubuntu.com/ubuntu jammy/main amd64 libopengl-dev amd64 1.4.0-1 [3,400 B]\n",
      "Get:9 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglvnd-dev amd64 1.4.0-1 [3,162 B]\n",
      "Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgl1-mesa-dev amd64 23.2.1-1ubuntu3.1~22.04.3 [6,848 B]\n",
      "Get:11 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglu1-mesa amd64 9.0.2-1 [145 kB]\n",
      "Get:12 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglu1-mesa-dev amd64 9.0.2-1 [231 kB]\n",
      "Get:13 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxt-dev amd64 1:1.2.1-1 [396 kB]\n",
      "Get:14 http://archive.ubuntu.com/ubuntu jammy/universe amd64 freeglut3-dev amd64 2.8.1-6 [126 kB]\n",
      "Fetched 1,192 kB in 3s (421 kB/s)       \u001b[0m\u001b[33m\n",
      "Selecting previously unselected package freeglut3:amd64.\n",
      "(Reading database ... 121713 files and directories currently installed.)\n",
      "Preparing to unpack .../00-freeglut3_2.8.1-6_amd64.deb ...\n",
      "Unpacking freeglut3:amd64 (2.8.1-6) ...\n",
      "Selecting previously unselected package libglx-dev:amd64.\n",
      "Preparing to unpack .../01-libglx-dev_1.4.0-1_amd64.deb ...\n",
      "Unpacking libglx-dev:amd64 (1.4.0-1) ...\n",
      "Selecting previously unselected package libgl-dev:amd64.\n",
      "Preparing to unpack .../02-libgl-dev_1.4.0-1_amd64.deb ...\n",
      "Unpacking libgl-dev:amd64 (1.4.0-1) ...\n",
      "Selecting previously unselected package libglvnd-core-dev:amd64.\n",
      "Preparing to unpack .../03-libglvnd-core-dev_1.4.0-1_amd64.deb ...\n",
      "Unpacking libglvnd-core-dev:amd64 (1.4.0-1) ...\n",
      "Selecting previously unselected package libegl-dev:amd64.\n",
      "Preparing to unpack .../04-libegl-dev_1.4.0-1_amd64.deb ...\n",
      "Unpacking libegl-dev:amd64 (1.4.0-1) ...\n",
      "Selecting previously unselected package libgles1:amd64.\n",
      "Preparing to unpack .../05-libgles1_1.4.0-1_amd64.deb ...\n",
      "Unpacking libgles1:amd64 (1.4.0-1) ...\n",
      "Selecting previously unselected package libgles-dev:amd64.\n",
      "Preparing to unpack .../06-libgles-dev_1.4.0-1_amd64.deb ...\n",
      "Unpacking libgles-dev:amd64 (1.4.0-1) ...\n",
      "Selecting previously unselected package libopengl-dev:amd64.\n",
      "Preparing to unpack .../07-libopengl-dev_1.4.0-1_amd64.deb ...\n",
      "Unpacking libopengl-dev:amd64 (1.4.0-1) ...\n",
      "Selecting previously unselected package libglvnd-dev:amd64.\n",
      "Preparing to unpack .../08-libglvnd-dev_1.4.0-1_amd64.deb ...\n",
      "Unpacking libglvnd-dev:amd64 (1.4.0-1) ...\n",
      "Selecting previously unselected package libgl1-mesa-dev:amd64.\n",
      "Preparing to unpack .../09-libgl1-mesa-dev_23.2.1-1ubuntu3.1~22.04.3_amd64.deb ...\n",
      "Unpacking libgl1-mesa-dev:amd64 (23.2.1-1ubuntu3.1~22.04.3) ...\n",
      "Selecting previously unselected package libglu1-mesa:amd64.\n",
      "Preparing to unpack .../10-libglu1-mesa_9.0.2-1_amd64.deb ...\n",
      "Unpacking libglu1-mesa:amd64 (9.0.2-1) ...\n",
      "Selecting previously unselected package libglu1-mesa-dev:amd64.\n",
      "Preparing to unpack .../11-libglu1-mesa-dev_9.0.2-1_amd64.deb ...\n",
      "Unpacking libglu1-mesa-dev:amd64 (9.0.2-1) ...\n",
      "Selecting previously unselected package libxt-dev:amd64.\n",
      "Preparing to unpack .../12-libxt-dev_1%3a1.2.1-1_amd64.deb ...\n",
      "Unpacking libxt-dev:amd64 (1:1.2.1-1) ...\n",
      "Selecting previously unselected package freeglut3-dev:amd64.\n",
      "Preparing to unpack .../13-freeglut3-dev_2.8.1-6_amd64.deb ...\n",
      "Unpacking freeglut3-dev:amd64 (2.8.1-6) ...\n",
      "Setting up freeglut3:amd64 (2.8.1-6) ...\n",
      "Setting up libglvnd-core-dev:amd64 (1.4.0-1) ...\n",
      "Setting up libxt-dev:amd64 (1:1.2.1-1) ...\n",
      "Setting up libgles1:amd64 (1.4.0-1) ...\n",
      "Setting up libglx-dev:amd64 (1.4.0-1) ...\n",
      "Setting up libglu1-mesa:amd64 (9.0.2-1) ...\n",
      "Setting up libopengl-dev:amd64 (1.4.0-1) ...\n",
      "Setting up libgl-dev:amd64 (1.4.0-1) ...\n",
      "Setting up libegl-dev:amd64 (1.4.0-1) ...\n",
      "Setting up libglu1-mesa-dev:amd64 (9.0.2-1) ...\n",
      "Setting up libgles-dev:amd64 (1.4.0-1) ...\n",
      "Setting up libglvnd-dev:amd64 (1.4.0-1) ...\n",
      "Setting up libgl1-mesa-dev:amd64 (23.2.1-1ubuntu3.1~22.04.3) ...\n",
      "Setting up freeglut3-dev:amd64 (2.8.1-6) ...\n",
      "Processing triggers for man-db (2.10.2-1) ...\n",
      "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
      "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libumf.so.1 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
      "\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "xvfb is already the newest version (2:21.1.4-2ubuntu1.7~22.04.16).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 41 not upgraded.\n",
      "Collecting pyvirtualdisplay\n",
      "  Downloading PyVirtualDisplay-3.0-py3-none-any.whl.metadata (943 bytes)\n",
      "Collecting miniworld\n",
      "  Downloading miniworld-2.1.0-py3-none-any.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: gymnasium in /usr/local/lib/python3.12/dist-packages (1.2.2)\n",
      "Requirement already satisfied: numpy>=1.22.0 in /usr/local/lib/python3.12/dist-packages (from miniworld) (2.0.2)\n",
      "Collecting pyglet<2.0,>=1.5.27 (from miniworld)\n",
      "  Downloading pyglet-1.5.31-py3-none-any.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (3.1.2)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (4.15.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (0.0.4)\n",
      "Downloading PyVirtualDisplay-3.0-py3-none-any.whl (15 kB)\n",
      "Downloading miniworld-2.1.0-py3-none-any.whl (39.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.4/39.4 MB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyglet-1.5.31-py3-none-any.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m72.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyvirtualdisplay, pyglet, miniworld\n",
      "Successfully installed miniworld-2.1.0 pyglet-1.5.31 pyvirtualdisplay-3.0\n"
     ]
    }
   ],
   "source": [
    "!apt install freeglut3-dev\n",
    "!apt-get -y install xvfb\n",
    "!pip install pyvirtualdisplay miniworld gymnasium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82870787",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyvirtualdisplay import Display\n",
    "\n",
    "display = Display(visible=0, size=(800, 600))\n",
    "display.start()\n",
    "\n",
    "import os\n",
    "# Make sure we are NOT forcing pyglet headless; clear any leftovers\n",
    "os.environ.pop(\"PYGLET_HEADLESS\", None)\n",
    "os.environ.pop(\"MINIWORLD_HEADLESS\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b40b3d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: CUDA\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict, deque, OrderedDict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "import miniworld\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from PIL import Image\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Set device - M4 Mac should use MPS!\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "    print(f\"Using device: MPS (Apple Silicon GPU)\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"Using device: CUDA\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(f\"Using device: CPU (SLOW!)\")\n",
    "\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8357fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    3.1 Encoder: CNN + MLP\n",
    "    Input: o_t ∈ R^(3×64×64)\n",
    "    Output: e_t ∈ R^(d_e) (e.g., 128)\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_dim=128):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        # CNN: 4 conv layers, stride 2\n",
    "        # Channels: 32 → 64 → 128 → 256\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        # After 4 stride-2 convolutions: 64 → 32 → 16 → 8 → 4\n",
    "        # So spatial size is 4x4\n",
    "        self.flatten_dim = 256 * 4 * 4\n",
    "        \n",
    "        # MLP: 1024 → embedding_dim\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(self.flatten_dim, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, embedding_dim),\n",
    "        )\n",
    "    \n",
    "    def forward(self, obs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            obs: (B, 3, 64, 64) tensor\n",
    "        Returns:\n",
    "            e_t: (B, embedding_dim) tensor\n",
    "        \"\"\"\n",
    "        x = self.cnn(obs)\n",
    "        e_t = self.mlp(x)\n",
    "        return e_t\n",
    "\n",
    "\n",
    "class RSSM(nn.Module):\n",
    "    \"\"\"\n",
    "    3.2 RSSM latent dynamics\n",
    "    Maintains deterministic state h_t and stochastic state z_t\n",
    "    \"\"\"\n",
    "    def __init__(self, action_dim, embedding_dim=128, hidden_dim=200, stochastic_dim=64):\n",
    "        super(RSSM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.stochastic_dim = stochastic_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        # Prior network (prediction)\n",
    "        # Input: [z_{t-1}, a_{t-1}] → MLP → GRU → h_t\n",
    "        self.prior_mlp = nn.Sequential(\n",
    "            nn.Linear(stochastic_dim + action_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "        )\n",
    "        \n",
    "        self.gru = nn.GRUCell(hidden_dim, hidden_dim)\n",
    "        \n",
    "        # Prior: MLP(h_t) → μ_t^prior, log σ_t^prior\n",
    "        self.prior_mean = nn.Linear(hidden_dim, stochastic_dim)\n",
    "        self.prior_std = nn.Linear(hidden_dim, stochastic_dim)\n",
    "        \n",
    "        # Posterior network (correction)\n",
    "        # Input: [h_t, e_t] → MLP → μ_t^post, log σ_t^post\n",
    "        self.posterior_mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_dim + embedding_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "        )\n",
    "        \n",
    "        self.posterior_mean = nn.Linear(hidden_dim, stochastic_dim)\n",
    "        self.posterior_std = nn.Linear(hidden_dim, stochastic_dim)\n",
    "    \n",
    "    def prior(self, h_prev, z_prev, a_prev):\n",
    "        \"\"\"\n",
    "        Prior (prediction): p(z_t | h_{t-1}, z_{t-1}, a_{t-1})\n",
    "        \n",
    "        Args:\n",
    "            h_prev: (B, hidden_dim) previous deterministic state\n",
    "            z_prev: (B, stochastic_dim) previous stochastic state\n",
    "            a_prev: (B, action_dim) previous action\n",
    "        Returns:\n",
    "            h_t: (B, hidden_dim) new deterministic state\n",
    "            z_t_prior: (B, stochastic_dim) sampled prior stochastic state\n",
    "            prior_dist: Normal distribution for KL loss\n",
    "        \"\"\"\n",
    "        # Concat [z_{t-1}, a_{t-1}]\n",
    "        x = torch.cat([z_prev, a_prev], dim=-1)\n",
    "        \n",
    "        # MLP → GRU → h_t\n",
    "        x = self.prior_mlp(x)\n",
    "        h_t = self.gru(x, h_prev)\n",
    "        \n",
    "        # MLP(h_t) → μ_t^prior, log σ_t^prior\n",
    "        mean = self.prior_mean(h_t)\n",
    "        log_std = self.prior_std(h_t)\n",
    "        log_std = torch.clamp(log_std, min=-10, max=2)  # Clamp for stability\n",
    "        std = torch.exp(log_std)\n",
    "        \n",
    "        # Sample z_t^prior\n",
    "        prior_dist = Normal(mean, std)\n",
    "        z_t_prior = prior_dist.rsample()  # Reparameterization trick\n",
    "        \n",
    "        return h_t, z_t_prior, prior_dist\n",
    "    \n",
    "    def posterior(self, h_t, e_t):\n",
    "        \"\"\"\n",
    "        Posterior (correction): q(z_t | h_t, e_t)\n",
    "        \n",
    "        Args:\n",
    "            h_t: (B, hidden_dim) deterministic state\n",
    "            e_t: (B, embedding_dim) encoded observation\n",
    "        Returns:\n",
    "            z_t_post: (B, stochastic_dim) sampled posterior stochastic state\n",
    "            post_dist: Normal distribution for KL loss\n",
    "        \"\"\"\n",
    "        # Concat [h_t, e_t]\n",
    "        x = torch.cat([h_t, e_t], dim=-1)\n",
    "        \n",
    "        # MLP → μ_t^post, log σ_t^post\n",
    "        x = self.posterior_mlp(x)\n",
    "        mean = self.posterior_mean(x)\n",
    "        log_std = self.posterior_std(x)\n",
    "        log_std = torch.clamp(log_std, min=-10, max=2)  # Clamp for stability\n",
    "        std = torch.exp(log_std)\n",
    "        \n",
    "        # Sample z_t^post\n",
    "        post_dist = Normal(mean, std)\n",
    "        z_t_post = post_dist.rsample()  # Reparameterization trick\n",
    "        \n",
    "        return z_t_post, post_dist\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    3.3 Decoder: Image reconstruction\n",
    "    Input: [h_t, z_t]\n",
    "    Output: o_hat_t (same shape as input: 3×64×64)\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim=200, stochastic_dim=64):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.stochastic_dim = stochastic_dim\n",
    "        \n",
    "        # MLP: [h_t, z_t] → flattened features\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_dim + stochastic_dim, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 256 * 4 * 4),  # Reshape to 256×4×4\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        # 4 deconv layers\n",
    "        self.deconv = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 3, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Sigmoid(),  # Output in [0, 1]\n",
    "        )\n",
    "    \n",
    "    def forward(self, h_t, z_t):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            h_t: (B, hidden_dim) deterministic state\n",
    "            z_t: (B, stochastic_dim) stochastic state\n",
    "        Returns:\n",
    "            o_hat_t: (B, 3, 64, 64) reconstructed observation\n",
    "        \"\"\"\n",
    "        # Concat [h_t, z_t]\n",
    "        x = torch.cat([h_t, z_t], dim=-1)\n",
    "        \n",
    "        # MLP → reshape\n",
    "        x = self.mlp(x)\n",
    "        x = x.view(-1, 256, 4, 4)\n",
    "        \n",
    "        # Deconv layers\n",
    "        o_hat_t = self.deconv(x)\n",
    "        \n",
    "        return o_hat_t\n",
    "\n",
    "\n",
    "class RewardHead(nn.Module):\n",
    "    \"\"\"\n",
    "    3.4 Reward head\n",
    "    Input: [h_t, z_t]\n",
    "    Output: scalar r_hat_t\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim=200, stochastic_dim=64):\n",
    "        super(RewardHead, self).__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_dim + stochastic_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1),\n",
    "        )\n",
    "    \n",
    "    def forward(self, h_t, z_t):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            h_t: (B, hidden_dim) deterministic state\n",
    "            z_t: (B, stochastic_dim) stochastic state\n",
    "        Returns:\n",
    "            r_hat_t: (B, 1) predicted reward\n",
    "        \"\"\"\n",
    "        x = torch.cat([h_t, z_t], dim=-1)\n",
    "        r_hat_t = self.mlp(x)\n",
    "        return r_hat_t.squeeze(-1)  # (B,)\n",
    "\n",
    "\n",
    "class ValueHead(nn.Module):\n",
    "    \"\"\"\n",
    "    3.5 Value head\n",
    "    Input: [h_t, z_t]\n",
    "    Output: scalar v_hat_t\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim=200, stochastic_dim=64):\n",
    "        super(ValueHead, self).__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_dim + stochastic_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1),\n",
    "        )\n",
    "    \n",
    "    def forward(self, h_t, z_t):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            h_t: (B, hidden_dim) deterministic state\n",
    "            z_t: (B, stochastic_dim) stochastic state\n",
    "        Returns:\n",
    "            v_hat_t: (B, 1) predicted value\n",
    "        \"\"\"\n",
    "        x = torch.cat([h_t, z_t], dim=-1)\n",
    "        v_hat_t = self.mlp(x)\n",
    "        return v_hat_t.squeeze(-1)  # (B,)\n",
    "\n",
    "\n",
    "class PolicyPriorHead(nn.Module):\n",
    "    \"\"\"\n",
    "    3.6 Policy prior head (for MCTS)\n",
    "    Input: [h_t, z_t]\n",
    "    Output: policy prior π_θ(a|s) - logits over actions\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim=200, stochastic_dim=64, action_dim=3):\n",
    "        super(PolicyPriorHead, self).__init__()\n",
    "        self.action_dim = action_dim\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_dim + stochastic_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, action_dim),\n",
    "        )\n",
    "    \n",
    "    def forward(self, h_t, z_t):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            h_t: (B, hidden_dim) deterministic state\n",
    "            z_t: (B, stochastic_dim) stochastic state\n",
    "        Returns:\n",
    "            logits: (B, action_dim) action logits\n",
    "            probs: (B, action_dim) action probabilities (softmax)\n",
    "        \"\"\"\n",
    "        x = torch.cat([h_t, z_t], dim=-1)\n",
    "        logits = self.mlp(x)\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        return logits, probs\n",
    "\n",
    "\n",
    "class WorldModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete World Model integrating all components:\n",
    "    - Encoder\n",
    "    - RSSM (prior & posterior)\n",
    "    - Decoder\n",
    "    - Reward head\n",
    "    - Value head\n",
    "    - Policy prior head\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        action_dim=3,\n",
    "        embedding_dim=128,\n",
    "        hidden_dim=200,\n",
    "        stochastic_dim=64,\n",
    "        action_space_size=3,\n",
    "    ):\n",
    "        super(WorldModel, self).__init__()\n",
    "        \n",
    "        self.action_dim = action_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.stochastic_dim = stochastic_dim\n",
    "        \n",
    "        # Components\n",
    "        self.encoder = Encoder(embedding_dim=embedding_dim)\n",
    "        self.rssm = RSSM(\n",
    "            action_dim=action_dim,\n",
    "            embedding_dim=embedding_dim,\n",
    "            hidden_dim=hidden_dim,\n",
    "            stochastic_dim=stochastic_dim,\n",
    "        )\n",
    "        self.decoder = Decoder(\n",
    "            hidden_dim=hidden_dim,\n",
    "            stochastic_dim=stochastic_dim,\n",
    "        )\n",
    "        self.reward_head = RewardHead(\n",
    "            hidden_dim=hidden_dim,\n",
    "            stochastic_dim=stochastic_dim,\n",
    "        )\n",
    "        self.value_head = ValueHead(\n",
    "            hidden_dim=hidden_dim,\n",
    "            stochastic_dim=stochastic_dim,\n",
    "        )\n",
    "        self.policy_prior_head = PolicyPriorHead(\n",
    "            hidden_dim=hidden_dim,\n",
    "            stochastic_dim=stochastic_dim,\n",
    "            action_dim=action_space_size,\n",
    "        )\n",
    "    \n",
    "    def forward(self, obs, action, h_prev=None, z_prev=None, use_posterior=True):\n",
    "        \"\"\"\n",
    "        Forward pass through the world model.\n",
    "        \n",
    "        Args:\n",
    "            obs: (B, 3, 64, 64) current observation\n",
    "            action: (B, action_dim) previous action (one-hot or embedding)\n",
    "            h_prev: (B, hidden_dim) previous deterministic state (None for first step)\n",
    "            z_prev: (B, stochastic_dim) previous stochastic state (None for first step)\n",
    "            use_posterior: bool, if True use posterior (training), else use prior (imagination)\n",
    "        \n",
    "        Returns:\n",
    "            dict with all outputs\n",
    "        \"\"\"\n",
    "        batch_size = obs.shape[0]\n",
    "        device = obs.device\n",
    "        \n",
    "        # Initialize states if None\n",
    "        if h_prev is None:\n",
    "            h_prev = torch.zeros(batch_size, self.hidden_dim, device=device)\n",
    "        if z_prev is None:\n",
    "            z_prev = torch.zeros(batch_size, self.stochastic_dim, device=device)\n",
    "        \n",
    "        # Encode observation\n",
    "        e_t = self.encoder(obs)\n",
    "        \n",
    "        # RSSM: Prior prediction\n",
    "        h_t, z_t_prior, prior_dist = self.rssm.prior(h_prev, z_prev, action)\n",
    "        \n",
    "        # RSSM: Posterior correction (if training)\n",
    "        if use_posterior:\n",
    "            z_t, post_dist = self.rssm.posterior(h_t, e_t)\n",
    "        else:\n",
    "            z_t = z_t_prior\n",
    "            post_dist = None\n",
    "        \n",
    "        # Decoder: Image reconstruction\n",
    "        o_hat_t = self.decoder(h_t, z_t)\n",
    "        \n",
    "        # Reward head\n",
    "        r_hat_t = self.reward_head(h_t, z_t)\n",
    "        \n",
    "        # Value head\n",
    "        v_hat_t = self.value_head(h_t, z_t)\n",
    "        \n",
    "        # Policy prior head\n",
    "        policy_logits, policy_probs = self.policy_prior_head(h_t, z_t)\n",
    "        \n",
    "        return {\n",
    "            'e_t': e_t,\n",
    "            'h_t': h_t,\n",
    "            'z_t': z_t,\n",
    "            'z_t_prior': z_t_prior,\n",
    "            'prior_dist': prior_dist,\n",
    "            'post_dist': post_dist,\n",
    "            'o_hat_t': o_hat_t,\n",
    "            'r_hat_t': r_hat_t,\n",
    "            'v_hat_t': v_hat_t,\n",
    "            'policy_logits': policy_logits,\n",
    "            'policy_probs': policy_probs,\n",
    "        }\n",
    "    \n",
    "    def compute_loss(self, obs, action, reward, value_targets=None, h_prev=None, z_prev=None, \n",
    "                     recon_loss_weight=1.0, reward_loss_weight=1.0, kl_loss_weight=0.1, value_loss_weight=1.0):\n",
    "        \"\"\"\n",
    "        Compute training losses:\n",
    "        - Reconstruction loss (MSE)\n",
    "        - Reward prediction loss (MSE)\n",
    "        - KL divergence loss (prior vs posterior)\n",
    "        - Value prediction loss (MSE) - optional\n",
    "        \n",
    "        Args:\n",
    "            obs: (B, 3, 64, 64) observation\n",
    "            action: (B, action_dim) action\n",
    "            reward: (B,) true reward\n",
    "            value_targets: (B,) n-step return targets (optional)\n",
    "            h_prev, z_prev: previous states\n",
    "            recon_loss_weight: weight for reconstruction loss\n",
    "            reward_loss_weight: weight for reward loss\n",
    "            kl_loss_weight: weight for KL loss\n",
    "            value_loss_weight: weight for value loss\n",
    "        \n",
    "        Returns:\n",
    "            dict with losses\n",
    "        \"\"\"\n",
    "        # Forward pass with posterior (training)\n",
    "        outputs = self.forward(obs, action, h_prev, z_prev, use_posterior=True)\n",
    "        \n",
    "        # Reconstruction loss (MSE)\n",
    "        recon_loss = F.mse_loss(outputs['o_hat_t'], obs)\n",
    "        \n",
    "        # Reward prediction loss (MSE)\n",
    "        reward_loss = F.mse_loss(outputs['r_hat_t'], reward)\n",
    "        \n",
    "        # KL divergence loss (KL(q(z_t|h_t,e_t) || p(z_t|h_{t-1},z_{t-1},a_{t-1})))\n",
    "        kl_loss = 0.0\n",
    "        if outputs['prior_dist'] is not None and outputs['post_dist'] is not None:\n",
    "            kl_loss = torch.distributions.kl.kl_divergence(\n",
    "                outputs['post_dist'], outputs['prior_dist']\n",
    "            ).mean()\n",
    "        \n",
    "        # Value prediction loss (MSE) - if targets provided\n",
    "        value_loss = torch.tensor(0.0, device=obs.device)\n",
    "        if value_targets is not None:\n",
    "            value_loss = F.mse_loss(outputs['v_hat_t'], value_targets)\n",
    "        \n",
    "        # Total loss\n",
    "        total_loss = (\n",
    "            recon_loss_weight * recon_loss +\n",
    "            reward_loss_weight * reward_loss +\n",
    "            kl_loss_weight * kl_loss +\n",
    "            value_loss_weight * value_loss\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'total_loss': total_loss,\n",
    "            'recon_loss': recon_loss,\n",
    "            'reward_loss': reward_loss,\n",
    "            'kl_loss': kl_loss,\n",
    "            'value_loss': value_loss,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc032a8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  env_name: MiniWorld-OneRoom-v0\n",
      "  obs_size: (64, 64)\n",
      "  action_dim: 3\n",
      "  embedding_dim: 128\n",
      "  hidden_dim: 200\n",
      "  stochastic_dim: 64\n",
      "  batch_size: 16\n",
      "  seq_length: 10\n",
      "  learning_rate: 0.0003\n",
      "  num_collection_episodes: 100\n",
      "  num_training_steps: 10000\n",
      "  collect_every_n_steps: 50\n",
      "  lambda_rec: 10.0\n",
      "  lambda_kl_start: 0.0\n",
      "  lambda_kl_end: 0.1\n",
      "  kl_anneal_steps: 4000\n",
      "  lambda_reward: 1.0\n",
      "  lambda_value: 1.0\n",
      "  free_nats: 1.0\n",
      "  n_step: 5\n",
      "  gamma: 0.99\n",
      "  epsilon: 0.3\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "config = {\n",
    "    'env_name': 'MiniWorld-OneRoom-v0',\n",
    "    'obs_size': (64, 64),\n",
    "    'action_dim': 3,  # OneRoom has 3 actions\n",
    "    'embedding_dim': 128,\n",
    "    'hidden_dim': 200,\n",
    "    'stochastic_dim': 64,\n",
    "    \n",
    "    # Training hyperparameters\n",
    "    'batch_size': 16,\n",
    "    'seq_length': 10,\n",
    "    'learning_rate': 3e-4,\n",
    "    'num_collection_episodes': 100,\n",
    "    'num_training_steps': 10000,\n",
    "    'collect_every_n_steps': 50,\n",
    "    \n",
    "    # Loss weights / regularization balance\n",
    "    'lambda_rec': 10.0,   # Reduce recon dominance so KL can matter\n",
    "    'lambda_kl_start': 0.0,\n",
    "    'lambda_kl_end': 0.10,  # Stronger KL at the end\n",
    "    'kl_anneal_steps': 4000,  # Faster ramp-up\n",
    "    'lambda_reward': 1.0,\n",
    "    'lambda_value': 1.0,\n",
    "    'free_nats': 1.0,  # Small per-dim allowance before KL is penalized\n",
    "    \n",
    "    # N-step returns\n",
    "    'n_step': 5,\n",
    "    'gamma': 0.99,\n",
    "    \n",
    "    # Exploration\n",
    "    'epsilon': 0.3,  # For epsilon-greedy heuristic policy\n",
    "}\n",
    "\n",
    "print(\"Configuration:\")\n",
    "for k, v in config.items():\n",
    "    print(f\"  {k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "524a9e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"\n",
    "    Replay buffer for storing trajectories.\n",
    "    Stores: (o_t, a_t, r_t, o_{t+1}, done_t)\n",
    "    \"\"\"\n",
    "    def __init__(self, capacity=10000):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def add(self, obs, action, reward, next_obs, done):\n",
    "        \"\"\"Add a single transition\"\"\"\n",
    "        self.buffer.append({\n",
    "            'obs': obs,\n",
    "            'action': action,\n",
    "            'reward': reward,\n",
    "            'next_obs': next_obs,\n",
    "            'done': done,\n",
    "        })\n",
    "    \n",
    "    def add_trajectory(self, trajectory):\n",
    "        \"\"\"Add a full trajectory\"\"\"\n",
    "        for transition in trajectory:\n",
    "            self.add(**transition)\n",
    "    \n",
    "    def sample_sequences(self, batch_size, seq_length):\n",
    "        \"\"\"\n",
    "        Sample sequences of length seq_length from the buffer.\n",
    "        Returns sequences of (obs, action, reward, done)\n",
    "        \"\"\"\n",
    "        if len(self.buffer) < seq_length:\n",
    "            return None\n",
    "        \n",
    "        # Sample random starting indices\n",
    "        max_start = len(self.buffer) - seq_length\n",
    "        starts = np.random.randint(0, max_start, size=batch_size)\n",
    "        \n",
    "        obs_seq = []\n",
    "        action_seq = []\n",
    "        reward_seq = []\n",
    "        done_seq = []\n",
    "        \n",
    "        for start in starts:\n",
    "            # Extract sequence\n",
    "            seq = [self.buffer[start + i] for i in range(seq_length)]\n",
    "            \n",
    "            obs_seq.append([s['obs'] for s in seq])\n",
    "            action_seq.append([s['action'] for s in seq])\n",
    "            reward_seq.append([s['reward'] for s in seq])\n",
    "            done_seq.append([s['done'] for s in seq])\n",
    "        \n",
    "        # Convert to tensors\n",
    "        # obs: (batch, seq, 3, 64, 64)\n",
    "        obs_tensor = torch.stack([torch.stack([torch.tensor(o, dtype=torch.float32) for o in obs]) for obs in obs_seq])\n",
    "        # action: (batch, seq, action_dim) - one-hot\n",
    "        action_tensor = torch.stack([torch.stack([F.one_hot(torch.tensor(a, dtype=torch.long), config['action_dim']).float() for a in action]) for action in action_seq])\n",
    "        # reward: (batch, seq)\n",
    "        reward_tensor = torch.stack([torch.tensor(reward, dtype=torch.float32) for reward in reward_seq])\n",
    "        # done: (batch, seq)\n",
    "        done_tensor = torch.stack([torch.tensor(done, dtype=torch.float32) for done in done_seq])\n",
    "        \n",
    "        return obs_tensor, action_tensor, reward_tensor, done_tensor\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2ce9528",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_obs(obs):\n",
    "    \"\"\"\n",
    "    Preprocess observation to (3, 64, 64) tensor.\n",
    "    MiniWorld returns (H, W, 3) numpy array.\n",
    "    \"\"\"\n",
    "    if isinstance(obs, np.ndarray):\n",
    "        # Convert to PIL Image if needed\n",
    "        if obs.dtype != np.uint8:\n",
    "            obs = (obs * 255).astype(np.uint8)\n",
    "        img = Image.fromarray(obs)\n",
    "    else:\n",
    "        img = obs\n",
    "    \n",
    "    # Resize to 64x64\n",
    "    img = img.resize((64, 64), Image.LANCZOS)\n",
    "    \n",
    "    # Convert to numpy and normalize to [0, 1]\n",
    "    img_array = np.array(img).astype(np.float32) / 255.0\n",
    "    \n",
    "    # Convert HWC to CHW: (64, 64, 3) -> (3, 64, 64)\n",
    "    img_array = np.transpose(img_array, (2, 0, 1))\n",
    "    \n",
    "    return img_array\n",
    "\n",
    "\n",
    "def heuristic_policy(env, epsilon=0.3):\n",
    "    \"\"\"\n",
    "    Simple heuristic policy: prefers moving forward (action 0), \n",
    "    occasionally takes random actions.\n",
    "    \"\"\"\n",
    "    if np.random.random() < epsilon:\n",
    "        return env.action_space.sample()\n",
    "    else:\n",
    "        return 0  # move_forward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d37f4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_trajectory(env, policy_fn, max_steps=500):\n",
    "    \"\"\"\n",
    "    Collect a single trajectory using the exploration policy.\n",
    "    \n",
    "    Returns:\n",
    "        trajectory: list of (obs, action, reward, next_obs, done)\n",
    "    \"\"\"\n",
    "    obs, info = env.reset()\n",
    "    obs = preprocess_obs(obs)\n",
    "    \n",
    "    trajectory = []\n",
    "    total_reward = 0\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        # Get action from policy\n",
    "        action = policy_fn(env, config['epsilon'])\n",
    "        \n",
    "        # Take step\n",
    "        next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "        next_obs = preprocess_obs(next_obs)\n",
    "        done = terminated or truncated\n",
    "        \n",
    "        # Store transition\n",
    "        trajectory.append({\n",
    "            'obs': obs.copy(),\n",
    "            'action': action,\n",
    "            'reward': float(reward),\n",
    "            'next_obs': next_obs.copy(),\n",
    "            'done': float(done),\n",
    "        })\n",
    "        \n",
    "        total_reward += reward\n",
    "        obs = next_obs\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    return trajectory, total_reward, len(trajectory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aac7f01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_n_step_returns(rewards, dones, values, gamma=0.99, n_step=5):\n",
    "    \"\"\"\n",
    "    Optimized vectorized n-step returns computation.\n",
    "    G_t = r_t + γ*r_{t+1} + ... + γ^{n-1}*r_{t+n-1} + γ^n * V_{t+n}\n",
    "    \n",
    "    Args:\n",
    "        rewards: (batch, seq) tensor of rewards\n",
    "        dones: (batch, seq) tensor of done flags\n",
    "        values: (batch, seq) tensor of predicted values (for bootstrapping) - should be detached\n",
    "        gamma: discount factor\n",
    "        n_step: number of steps for n-step return\n",
    "    \n",
    "    Returns:\n",
    "        returns: (batch, seq) tensor of n-step returns\n",
    "    \"\"\"\n",
    "    batch_size, seq_length = rewards.shape\n",
    "    device = rewards.device\n",
    "    values = values.detach()\n",
    "    \n",
    "    # Pre-compute discount factors\n",
    "    discounts = gamma ** torch.arange(n_step + 1, device=device, dtype=torch.float32)\n",
    "    \n",
    "    returns = torch.zeros_like(rewards)\n",
    "    \n",
    "    # Vectorized computation per timestep\n",
    "    for t in range(seq_length):\n",
    "        # Get rewards for next n steps\n",
    "        end_idx = min(t + n_step, seq_length)\n",
    "        n_rewards = rewards[:, t:end_idx]  # (B, n_actual)\n",
    "        n_dones = dones[:, t:end_idx]  # (B, n_actual)\n",
    "        \n",
    "        n_actual = n_rewards.shape[1]\n",
    "        disc = discounts[:n_actual].unsqueeze(0)  # (1, n_actual)\n",
    "        \n",
    "        # Mask out rewards after done (cumulative product to stop after first done)\n",
    "        done_mask = torch.cumprod(1.0 - n_dones, dim=1)  # (B, n_actual)\n",
    "        masked_rewards = n_rewards * done_mask\n",
    "        \n",
    "        # Sum discounted rewards\n",
    "        reward_sum = (masked_rewards * disc).sum(dim=1)  # (B,)\n",
    "        \n",
    "        # Bootstrap value\n",
    "        if t + n_step < seq_length:\n",
    "            # Use value at t+n_step if not done\n",
    "            bootstrap = discounts[n_step] * values[:, t + n_step] * (1.0 - dones[:, t + n_step])\n",
    "        else:\n",
    "            # Use last value if sequence ended\n",
    "            bootstrap = discounts[n_actual] * values[:, -1] * (1.0 - dones[:, -1])\n",
    "        \n",
    "        returns[:, t] = reward_sum + bootstrap\n",
    "    \n",
    "    return returns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ab911ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Falling back to num_samples=4\n",
      "Falling back to num_samples=4\n",
      "Environment: MiniWorld-OneRoom-v0\n",
      "Action space: Discrete(3)\n",
      "Observation space: Box(0, 255, (60, 80, 3), uint8)\n",
      "\n",
      "World Model initialized:\n",
      "  Total parameters: 11,030,960\n",
      "\n",
      "Initialization complete!\n"
     ]
    }
   ],
   "source": [
    "# Initialize environment\n",
    "env = gym.make(config['env_name'], render_mode='rgb_array')\n",
    "print(f\"Environment: {config['env_name']}\")\n",
    "print(f\"Action space: {env.action_space}\")\n",
    "print(f\"Observation space: {env.observation_space}\")\n",
    "\n",
    "# Initialize world model\n",
    "model = WorldModel(\n",
    "    action_dim=config['action_dim'],\n",
    "    embedding_dim=config['embedding_dim'],\n",
    "    hidden_dim=config['hidden_dim'],\n",
    "    stochastic_dim=config['stochastic_dim'],\n",
    "    action_space_size=config['action_dim'],\n",
    ").to(device)\n",
    "\n",
    "print(f\"\\nWorld Model initialized:\")\n",
    "print(f\"  Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'])\n",
    "\n",
    "# Initialize replay buffer\n",
    "replay_buffer = ReplayBuffer(capacity=10000)\n",
    "\n",
    "print(\"\\nInitialization complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6996eb",
   "metadata": {},
   "source": [
    "## 4.1 Data Collection Phase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d835f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting initial trajectories...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting: 100%|██████████| 100/100 [00:26<00:00,  3.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collected 10000 transitions\n",
      "Average reward: 0.02 ± 0.14\n",
      "Average length: 176.42 ± 25.06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Initial data collection\n",
    "print(\"Collecting initial trajectories...\")\n",
    "episode_rewards = []\n",
    "episode_lengths = []\n",
    "\n",
    "for episode in tqdm(range(config['num_collection_episodes']), desc=\"Collecting\"):\n",
    "    trajectory, total_reward, traj_length = collect_trajectory(\n",
    "        env, heuristic_policy, max_steps=500\n",
    "    )\n",
    "    replay_buffer.add_trajectory(trajectory)\n",
    "    episode_rewards.append(total_reward)\n",
    "    episode_lengths.append(traj_length)\n",
    "\n",
    "print(f\"\\nCollected {len(replay_buffer)} transitions\")\n",
    "print(f\"Average reward: {np.mean(episode_rewards):.2f} ± {np.std(episode_rewards):.2f}\")\n",
    "print(f\"Average length: {np.mean(episode_lengths):.2f} ± {np.std(episode_lengths):.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56d0d5d",
   "metadata": {},
   "source": [
    "## 4.2 Training Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5902fb42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training with KL annealing and free bits...\n",
      "KL weight: 0.000 -> 0.100 over 4000 steps\n",
      "Free nats: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  10%|▉         | 999/10000 [01:44<14:00, 10.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 1000/10000\n",
      "  Total loss: 0.0578\n",
      "  Recon: 0.0058, Reward: 0.0000\n",
      "  KL (raw/clamped): 0.8172/0.0115, KL weight: 0.0250\n",
      "  Value: 0.0000\n",
      "  Post/Prior std: 0.2532/0.8229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 2000/10000 [03:28<15:27,  8.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 2000/10000\n",
      "  Total loss: 0.0454\n",
      "  Recon: 0.0045, Reward: 0.0000\n",
      "  KL (raw/clamped): 0.7919/0.0038, KL weight: 0.0500\n",
      "  Value: 0.0000\n",
      "  Post/Prior std: 0.2871/0.8988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  30%|███       | 3000/10000 [05:13<11:16, 10.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 3000/10000\n",
      "  Total loss: 0.0388\n",
      "  Recon: 0.0039, Reward: 0.0001\n",
      "  KL (raw/clamped): 0.8078/0.0008, KL weight: 0.0750\n",
      "  Value: 0.0000\n",
      "  Post/Prior std: 0.2861/0.9210\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  40%|████      | 4000/10000 [06:58<09:29, 10.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 4000/10000\n",
      "  Total loss: 0.0538\n",
      "  Recon: 0.0054, Reward: 0.0000\n",
      "  KL (raw/clamped): 0.7484/0.0015, KL weight: 0.1000\n",
      "  Value: 0.0000\n",
      "  Post/Prior std: 0.2963/0.8818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  50%|█████     | 5000/10000 [08:43<07:45, 10.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 5000/10000\n",
      "  Total loss: 0.0286\n",
      "  Recon: 0.0029, Reward: 0.0000\n",
      "  KL (raw/clamped): 0.7714/0.0004, KL weight: 0.1000\n",
      "  Value: 0.0000\n",
      "  Post/Prior std: 0.3209/0.9876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  70%|███████   | 7000/10000 [12:14<04:45, 10.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 7000/10000\n",
      "  Total loss: 0.0467\n",
      "  Recon: 0.0047, Reward: 0.0000\n",
      "  KL (raw/clamped): 0.8111/0.0017, KL weight: 0.1000\n",
      "  Value: 0.0000\n",
      "  Post/Prior std: 0.3070/0.9819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  80%|████████  | 8000/10000 [13:57<03:44,  8.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 8000/10000\n",
      "  Total loss: 0.0364\n",
      "  Recon: 0.0036, Reward: 0.0000\n",
      "  KL (raw/clamped): 0.7622/0.0014, KL weight: 0.1000\n",
      "  Value: 0.0000\n",
      "  Post/Prior std: 0.3883/1.1406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  90%|█████████ | 9000/10000 [15:41<01:34, 10.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 9000/10000\n",
      "  Total loss: 0.0327\n",
      "  Recon: 0.0033, Reward: 0.0000\n",
      "  KL (raw/clamped): 0.7944/0.0018, KL weight: 0.1000\n",
      "  Value: 0.0000\n",
      "  Post/Prior std: 0.4092/1.2455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 10000/10000 [17:26<00:00,  9.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 10000/10000\n",
      "  Total loss: 0.0247\n",
      "  Recon: 0.0025, Reward: 0.0000\n",
      "  KL (raw/clamped): 0.7709/0.0020, KL weight: 0.1000\n",
      "  Value: 0.0000\n",
      "  Post/Prior std: 0.4420/1.2964\n",
      "\n",
      "Training complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Training loop with KL annealing and free bits\n",
    "model.train()\n",
    "losses_history = {\n",
    "    'total': [],\n",
    "    'recon': [],\n",
    "    'reward': [],\n",
    "    'kl': [],\n",
    "    'kl_raw': [],  # Track raw KL before free bits\n",
    "    'value': [],\n",
    "    'kl_weight': [],  # Track annealing schedule\n",
    "    'posterior_std': [],  # Track posterior collapse\n",
    "    'prior_std': [],\n",
    "}\n",
    "\n",
    "print(\"Starting training with KL annealing and free bits...\")\n",
    "print(f\"KL weight: {config['lambda_kl_start']:.3f} -> {config['lambda_kl_end']:.3f} over {config['kl_anneal_steps']} steps\")\n",
    "print(f\"Free nats: {config['free_nats']:.1f}\")\n",
    "\n",
    "for step in tqdm(range(config['num_training_steps']), desc=\"Training\"):\n",
    "    # Collect new data periodically\n",
    "    if step % config['collect_every_n_steps'] == 0 and step > 0:\n",
    "        trajectory, _, _ = collect_trajectory(env, heuristic_policy, max_steps=500)\n",
    "        replay_buffer.add_trajectory(trajectory)\n",
    "    \n",
    "    # KL annealing schedule\n",
    "    if step < config['kl_anneal_steps']:\n",
    "        kl_weight = config['lambda_kl_start'] + (config['lambda_kl_end'] - config['lambda_kl_start']) * (step / config['kl_anneal_steps'])\n",
    "    else:\n",
    "        kl_weight = config['lambda_kl_end']\n",
    "    \n",
    "    # Sample batch of sequences\n",
    "    batch = replay_buffer.sample_sequences(\n",
    "        batch_size=config['batch_size'],\n",
    "        seq_length=config['seq_length']\n",
    "    )\n",
    "    \n",
    "    if batch is None:\n",
    "        continue\n",
    "    \n",
    "    obs_seq, action_seq, reward_seq, done_seq = batch\n",
    "    obs_seq = obs_seq.to(device)  # (B, T, 3, 64, 64)\n",
    "    action_seq = action_seq.to(device)  # (B, T, action_dim)\n",
    "    reward_seq = reward_seq.to(device)  # (B, T)\n",
    "    done_seq = done_seq.to(device)  # (B, T)\n",
    "    \n",
    "    # Reshape for processing: (B*T, ...)\n",
    "    B, T = obs_seq.shape[:2]\n",
    "    obs_flat = obs_seq.view(B * T, 3, 64, 64)\n",
    "    action_flat = action_seq.view(B * T, config['action_dim'])\n",
    "    reward_flat = reward_seq.view(B * T)\n",
    "    \n",
    "    # Initialize states\n",
    "    h_prev = None\n",
    "    z_prev = None\n",
    "    \n",
    "    # Forward pass through sequence\n",
    "    all_outputs = []\n",
    "    for t in range(T):\n",
    "        obs_t = obs_seq[:, t]  # (B, 3, 64, 64)\n",
    "        action_t = action_seq[:, t]  # (B, action_dim)\n",
    "        \n",
    "        # Use previous action for RSSM (or zero for first step)\n",
    "        if t == 0:\n",
    "            action_prev = torch.zeros(B, config['action_dim'], device=device)\n",
    "        else:\n",
    "            action_prev = action_seq[:, t-1]\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(obs_t, action_prev, h_prev, z_prev, use_posterior=True)\n",
    "        all_outputs.append(outputs)\n",
    "        \n",
    "        # Update states for next step\n",
    "        h_prev = outputs['h_t']\n",
    "        z_prev = outputs['z_t']\n",
    "    \n",
    "    # Stack outputs: (B, T, ...)\n",
    "    o_hat_seq = torch.stack([o['o_hat_t'] for o in all_outputs], dim=1)  # (B, T, 3, 64, 64)\n",
    "    r_hat_seq = torch.stack([o['r_hat_t'] for o in all_outputs], dim=1)  # (B, T)\n",
    "    v_hat_seq = torch.stack([o['v_hat_t'] for o in all_outputs], dim=1)  # (B, T)\n",
    "    \n",
    "    # Compute n-step returns for value targets (detach values for bootstrapping)\n",
    "    with torch.no_grad():\n",
    "        value_targets = compute_n_step_returns(\n",
    "            reward_seq, done_seq, v_hat_seq.detach(),\n",
    "            gamma=config['gamma'],\n",
    "            n_step=config['n_step']\n",
    "        )\n",
    "    \n",
    "    # Compute losses\n",
    "    recon_loss = F.mse_loss(o_hat_seq, obs_seq)\n",
    "    reward_loss = F.mse_loss(r_hat_seq, reward_seq)\n",
    "    value_loss = F.mse_loss(v_hat_seq, value_targets)\n",
    "    \n",
    "    # KL loss with FREE BITS constraint (sum over sequence, mean over batch)\n",
    "    kl_loss_raw = torch.tensor(0.0, device=device)\n",
    "    kl_loss = torch.tensor(0.0, device=device)\n",
    "    posterior_stds = []\n",
    "    prior_stds = []\n",
    "    \n",
    "    for t in range(T):\n",
    "        if all_outputs[t]['prior_dist'] is not None and all_outputs[t]['post_dist'] is not None:\n",
    "            # Per-dimension KL divergence\n",
    "            kl_per_dim = torch.distributions.kl.kl_divergence(\n",
    "                all_outputs[t]['post_dist'],\n",
    "                all_outputs[t]['prior_dist']\n",
    "            )  # (B, stochastic_dim)\n",
    "            \n",
    "            # Raw KL (for logging)\n",
    "            kl_t_raw = kl_per_dim.mean()\n",
    "            kl_loss_raw += kl_t_raw\n",
    "            \n",
    "            # Free bits: don't penalize the first `free_nats` nats per dim\n",
    "            # Standard form: max(kl_per_dim - free_nats, 0)\n",
    "            free_nats = config['free_nats']\n",
    "            kl_per_dim_clamped = torch.clamp(kl_per_dim - free_nats, min=0.0)\n",
    "            kl_t = kl_per_dim_clamped.mean()\n",
    "            kl_loss += kl_t\n",
    "            \n",
    "            # Track std for diagnosing posterior collapse\n",
    "            posterior_stds.append(all_outputs[t]['post_dist'].stddev.mean().item())\n",
    "            prior_stds.append(all_outputs[t]['prior_dist'].stddev.mean().item())\n",
    "    \n",
    "    kl_loss = kl_loss / T\n",
    "    kl_loss_raw = kl_loss_raw / T\n",
    "    \n",
    "    # Total loss\n",
    "    total_loss = (\n",
    "        config['lambda_rec'] * recon_loss +\n",
    "        kl_weight * kl_loss +\n",
    "        config['lambda_reward'] * reward_loss +\n",
    "        config['lambda_value'] * value_loss\n",
    "    )\n",
    "    \n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Log losses\n",
    "    losses_history['total'].append(total_loss.item())\n",
    "    losses_history['recon'].append(recon_loss.item())\n",
    "    losses_history['reward'].append(reward_loss.item())\n",
    "    losses_history['kl'].append(kl_loss.item())\n",
    "    losses_history['kl_raw'].append(kl_loss_raw.item())\n",
    "    losses_history['value'].append(value_loss.item())\n",
    "    losses_history['kl_weight'].append(kl_weight)\n",
    "    losses_history['posterior_std'].append(np.mean(posterior_stds) if posterior_stds else 0)\n",
    "    losses_history['prior_std'].append(np.mean(prior_stds) if prior_stds else 0)\n",
    "    \n",
    "    # Print progress (less frequently to reduce overhead)\n",
    "    if (step + 1) % 1000 == 0:\n",
    "        print(f\"\\nStep {step + 1}/{config['num_training_steps']}\")\n",
    "        print(f\"  Total loss: {total_loss.item():.4f}\")\n",
    "        print(f\"  Recon: {recon_loss.item():.4f}, Reward: {reward_loss.item():.4f}\")\n",
    "        print(f\"  KL (raw/clamped): {kl_loss_raw.item():.4f}/{kl_loss.item():.4f}, KL weight: {kl_weight:.4f}\")\n",
    "        print(f\"  Value: {value_loss.item():.4f}\")\n",
    "        print(f\"  Post/Prior std: {np.mean(posterior_stds):.4f}/{np.mean(prior_stds):.4f}\")\n",
    "\n",
    "print(\"\\nTraining complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019400b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training losses with enhanced diagnostics\n",
    "fig, axes = plt.subplots(3, 3, figsize=(18, 12))\n",
    "\n",
    "# Row 1: Main losses\n",
    "axes[0, 0].plot(losses_history['total'])\n",
    "axes[0, 0].set_title('Total Loss')\n",
    "axes[0, 0].set_xlabel('Step')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].grid(True)\n",
    "\n",
    "axes[0, 1].plot(losses_history['recon'])\n",
    "axes[0, 1].set_title('Reconstruction Loss')\n",
    "axes[0, 1].set_xlabel('Step')\n",
    "axes[0, 1].set_ylabel('Loss')\n",
    "axes[0, 1].grid(True)\n",
    "\n",
    "axes[0, 2].plot(losses_history['reward'])\n",
    "axes[0, 2].set_title('Reward Prediction Loss')\n",
    "axes[0, 2].set_xlabel('Step')\n",
    "axes[0, 2].set_ylabel('Loss')\n",
    "axes[0, 2].grid(True)\n",
    "\n",
    "# Row 2: KL diagnostics\n",
    "axes[1, 0].plot(losses_history['kl_raw'], label='Raw KL', alpha=0.7)\n",
    "axes[1, 0].plot(losses_history['kl'], label='Clamped KL (free bits)', alpha=0.7)\n",
    "axes[1, 0].set_title('KL Divergence Loss')\n",
    "axes[1, 0].set_xlabel('Step')\n",
    "axes[1, 0].set_ylabel('Loss')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True)\n",
    "\n",
    "axes[1, 1].plot(losses_history['kl_weight'])\n",
    "axes[1, 1].set_title('KL Weight (Annealing Schedule)')\n",
    "axes[1, 1].set_xlabel('Step')\n",
    "axes[1, 1].set_ylabel('Weight')\n",
    "axes[1, 1].grid(True)\n",
    "\n",
    "axes[1, 2].plot(losses_history['value'])\n",
    "axes[1, 2].set_title('Value Prediction Loss')\n",
    "axes[1, 2].set_xlabel('Step')\n",
    "axes[1, 2].set_ylabel('Loss')\n",
    "axes[1, 2].grid(True)\n",
    "\n",
    "# Row 3: Posterior collapse diagnostics\n",
    "axes[2, 0].plot(losses_history['posterior_std'], label='Posterior', alpha=0.7)\n",
    "axes[2, 0].plot(losses_history['prior_std'], label='Prior', alpha=0.7)\n",
    "axes[2, 0].set_title('Latent Distribution Std Devs')\n",
    "axes[2, 0].set_xlabel('Step')\n",
    "axes[2, 0].set_ylabel('Std Dev')\n",
    "axes[2, 0].legend()\n",
    "axes[2, 0].grid(True)\n",
    "axes[2, 0].axhline(y=0.1, color='r', linestyle='--', alpha=0.3, label='Collapse threshold')\n",
    "\n",
    "# Histogram of final losses\n",
    "axes[2, 1].hist(losses_history['recon'][-1000:], bins=30, alpha=0.7)\n",
    "axes[2, 1].set_title('Recent Reconstruction Loss Distribution')\n",
    "axes[2, 1].set_xlabel('Loss')\n",
    "axes[2, 1].set_ylabel('Frequency')\n",
    "axes[2, 1].grid(True)\n",
    "\n",
    "# Histogram of KL\n",
    "axes[2, 2].hist(losses_history['kl_raw'][-1000:], bins=30, alpha=0.7, label='Raw')\n",
    "axes[2, 2].hist(losses_history['kl'][-1000:], bins=30, alpha=0.5, label='Clamped')\n",
    "axes[2, 2].set_title('Recent KL Loss Distribution')\n",
    "axes[2, 2].set_xlabel('Loss')\n",
    "axes[2, 2].set_ylabel('Frequency')\n",
    "axes[2, 2].legend()\n",
    "axes[2, 2].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\n=== Training Summary ===\")\n",
    "print(f\"Final reconstruction loss: {losses_history['recon'][-1]:.6f}\")\n",
    "print(f\"Final KL (raw/clamped): {losses_history['kl_raw'][-1]:.6f} / {losses_history['kl'][-1]:.6f}\")\n",
    "print(f\"Final posterior std: {losses_history['posterior_std'][-1]:.6f}\")\n",
    "print(f\"Final prior std: {losses_history['prior_std'][-1]:.6f}\")\n",
    "print(f\"\\nPosterior collapse check:\")\n",
    "if losses_history['posterior_std'][-1] < 0.1:\n",
    "    print(\"  ⚠️ WARNING: Posterior may have collapsed (std < 0.1)\")\n",
    "elif losses_history['posterior_std'][-1] > 0.5:\n",
    "    print(\"  ✅ GOOD: Posterior is active (std > 0.5)\")\n",
    "else:\n",
    "    print(\"  ⚠️ MARGINAL: Posterior std is low but not collapsed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b28e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize reconstructions with detailed statistics\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Sample a sequence\n",
    "    batch = replay_buffer.sample_sequences(batch_size=1, seq_length=10)\n",
    "    if batch is not None:\n",
    "        obs_seq, action_seq, reward_seq, done_seq = batch\n",
    "        obs_seq = obs_seq.to(device)\n",
    "        action_seq = action_seq.to(device)\n",
    "        \n",
    "        # Reconstruct\n",
    "        h_prev = None\n",
    "        z_prev = None\n",
    "        reconstructions = []\n",
    "        \n",
    "        for t in range(min(10, obs_seq.shape[1])):\n",
    "            obs_t = obs_seq[:, t]\n",
    "            if t == 0:\n",
    "                action_prev = torch.zeros(1, config['action_dim'], device=device)\n",
    "            else:\n",
    "                action_prev = action_seq[:, t-1]\n",
    "            \n",
    "            outputs = model(obs_t, action_prev, h_prev, z_prev, use_posterior=True)\n",
    "            reconstructions.append(outputs['o_hat_t'])\n",
    "            h_prev = outputs['h_t']\n",
    "            z_prev = outputs['z_t']\n",
    "        \n",
    "        # Plot original vs reconstructed\n",
    "        fig, axes = plt.subplots(2, 10, figsize=(20, 4))\n",
    "        for t in range(min(10, len(reconstructions))):\n",
    "            # Original\n",
    "            orig = obs_seq[0, t].cpu().numpy().transpose(1, 2, 0)\n",
    "            axes[0, t].imshow(np.clip(orig, 0, 1))\n",
    "            axes[0, t].set_title(f'Original t={t}')\n",
    "            axes[0, t].axis('off')\n",
    "            \n",
    "            # Reconstructed\n",
    "            recon = reconstructions[t][0].cpu().numpy().transpose(1, 2, 0)\n",
    "            axes[1, t].imshow(np.clip(recon, 0, 1))\n",
    "            axes[1, t].set_title(f'Reconstructed t={t}')\n",
    "            axes[1, t].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print detailed statistics for the first frame\n",
    "        print(\"\\n=== Reconstruction Statistics (t=0) ===\")\n",
    "        orig_0 = obs_seq[0, 0].cpu().numpy()\n",
    "        recon_0 = reconstructions[0][0].cpu().numpy()\n",
    "        \n",
    "        print(f\"Original image:\")\n",
    "        print(f\"  Mean: {orig_0.mean():.4f}, Std: {orig_0.std():.4f}\")\n",
    "        print(f\"  Min: {orig_0.min():.4f}, Max: {orig_0.max():.4f}\")\n",
    "        \n",
    "        print(f\"\\nReconstructed image:\")\n",
    "        print(f\"  Mean: {recon_0.mean():.4f}, Std: {recon_0.std():.4f}\")\n",
    "        print(f\"  Min: {recon_0.min():.4f}, Max: {recon_0.max():.4f}\")\n",
    "        \n",
    "        # Per-channel statistics\n",
    "        print(f\"\\nPer-channel (RGB) statistics:\")\n",
    "        for c, color in enumerate(['Red', 'Green', 'Blue']):\n",
    "            print(f\"  {color} - Orig: {orig_0[c].mean():.4f}, Recon: {recon_0[c].mean():.4f}\")\n",
    "        \n",
    "        # MSE\n",
    "        mse = np.mean((orig_0 - recon_0) ** 2)\n",
    "        print(f\"\\nMSE: {mse:.6f}\")\n",
    "        print(f\"RMSE: {np.sqrt(mse):.6f}\")\n",
    "        \n",
    "        # Check if reconstruction is constant\n",
    "        if recon_0.std() < 0.01:\n",
    "            print(\"\\n⚠️ WARNING: Reconstruction has very low variance - likely outputting constant values!\")\n",
    "            print(f\"This suggests the decoder is not learning. Check:\")\n",
    "            print(f\"  1. Is the reconstruction loss actually backpropagating?\")\n",
    "            print(f\"  2. Is the KL loss too strong initially?\")\n",
    "            print(f\"  3. Are the latent codes carrying information?\")\n",
    "        else:\n",
    "            print(f\"\\n✅ Reconstruction has variance: {recon_0.std():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b01816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'config': config,\n",
    "    'losses_history': losses_history,\n",
    "}, 'worldmodel_checkpoint.pth')\n",
    "print(\"Model saved to worldmodel_checkpoint.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab621649",
   "metadata": {},
   "source": [
    "MCTS         "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
