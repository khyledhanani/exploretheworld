{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# World Model Training Script\n",
        "\n",
        "This notebook implements training for the world model on MiniWorld OneRoom environment.\n",
        "\n",
        "## Training Pipeline:\n",
        "1. Data collection with simple exploration policy\n",
        "2. Replay buffer storage\n",
        "3. World model training with:\n",
        "   - Reconstruction loss\n",
        "   - KL divergence loss\n",
        "   - Reward prediction loss\n",
        "   - Value prediction loss (n-step returns)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: MPS (Apple Silicon GPU)\n",
            "Device: mps\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import gymnasium as gym\n",
        "import miniworld\n",
        "from collections import deque, defaultdict\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "from PIL import Image\n",
        "\n",
        "# Import world model\n",
        "import sys\n",
        "import os\n",
        "sys.path.append(os.path.dirname(os.path.abspath('')))\n",
        "from fun import WorldModel\n",
        "\n",
        "# Set device - M4 Mac should use MPS!\n",
        "if torch.backends.mps.is_available():\n",
        "    device = torch.device('mps')\n",
        "    print(f\"Using device: MPS (Apple Silicon GPU)\")\n",
        "elif torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "    print(f\"Using device: CUDA\")\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "    print(f\"Using device: CPU (SLOW!)\")\n",
        "\n",
        "print(f\"Device: {device}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Configuration:\n",
            "  env_name: MiniWorld-OneRoom-v0\n",
            "  obs_size: (64, 64)\n",
            "  action_dim: 4\n",
            "  embedding_dim: 128\n",
            "  hidden_dim: 200\n",
            "  stochastic_dim: 32\n",
            "  batch_size: 16\n",
            "  seq_length: 10\n",
            "  learning_rate: 0.0001\n",
            "  num_collection_episodes: 100\n",
            "  num_training_steps: 10000\n",
            "  collect_every_n_steps: 50\n",
            "  lambda_rec: 10.0\n",
            "  lambda_kl_start: 0.0\n",
            "  lambda_kl_end: 0.5\n",
            "  kl_anneal_steps: 2000\n",
            "  lambda_reward: 1.0\n",
            "  lambda_value: 1.0\n",
            "  free_nats: 3.0\n",
            "  n_step: 5\n",
            "  gamma: 0.99\n",
            "  epsilon: 0.3\n"
          ]
        }
      ],
      "source": [
        "# Configuration\n",
        "config = {\n",
        "    'env_name': 'MiniWorld-OneRoom-v0',\n",
        "    'obs_size': (64, 64),\n",
        "    'action_dim': 3,  # FIXED: OneRoom has 3 actions (was 4)\n",
        "    'embedding_dim': 128,\n",
        "    'hidden_dim': 200,\n",
        "    'stochastic_dim': 64,  # INCREASED: 32→64 for more capacity (harder to collapse)\n",
        "    \n",
        "    # Training hyperparameters\n",
        "    'batch_size': 16,  # Reduced for MPS memory efficiency\n",
        "    'seq_length': 10,  # Reduced from 50 for faster training (5x speedup!)\n",
        "    'learning_rate': 3e-4,  # INCREASED: 1e-4→3e-4 for faster learning\n",
        "    'num_collection_episodes': 100,  # Episodes to collect before training\n",
        "    'num_training_steps': 10000,\n",
        "    'collect_every_n_steps': 50,  # Collect less frequently (was 10)\n",
        "    \n",
        "    # Loss weights - VERY AGGRESSIVE anti-collapse settings\n",
        "    'lambda_rec': 100.0,  # MASSIVELY increased (was 10.0) - reconstruction is KING\n",
        "    'lambda_kl_start': 0.0,  # Start with zero KL weight\n",
        "    'lambda_kl_end': 0.01,  # VERY weak KL (was 0.5) - barely regularize\n",
        "    'kl_anneal_steps': 8000,  # VERY slow annealing (was 2000) - let reconstruction learn first\n",
        "    'lambda_reward': 1.0,\n",
        "    'lambda_value': 1.0,\n",
        "    'free_nats': 8.0,  # MUCH higher free bits (was 3.0) - strong protection\n",
        "    \n",
        "    # N-step returns\n",
        "    'n_step': 5,\n",
        "    'gamma': 0.99,\n",
        "    \n",
        "    # Exploration\n",
        "    'epsilon': 0.3,  # For epsilon-greedy heuristic policy\n",
        "}\n",
        "\n",
        "print(\"Configuration:\")\n",
        "for k, v in config.items():\n",
        "    print(f\"  {k}: {v}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "    \"\"\"\n",
        "    Replay buffer for storing trajectories.\n",
        "    Stores: (o_t, a_t, r_t, o_{t+1}, done_t)\n",
        "    \"\"\"\n",
        "    def __init__(self, capacity=10000):\n",
        "        self.capacity = capacity\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "    \n",
        "    def add(self, obs, action, reward, next_obs, done):\n",
        "        \"\"\"Add a single transition\"\"\"\n",
        "        self.buffer.append({\n",
        "            'obs': obs,\n",
        "            'action': action,\n",
        "            'reward': reward,\n",
        "            'next_obs': next_obs,\n",
        "            'done': done,\n",
        "        })\n",
        "    \n",
        "    def add_trajectory(self, trajectory):\n",
        "        \"\"\"Add a full trajectory\"\"\"\n",
        "        for transition in trajectory:\n",
        "            self.add(**transition)\n",
        "    \n",
        "    def sample_sequences(self, batch_size, seq_length):\n",
        "        \"\"\"\n",
        "        Sample sequences of length seq_length from the buffer.\n",
        "        Returns sequences of (obs, action, reward, done)\n",
        "        \"\"\"\n",
        "        if len(self.buffer) < seq_length:\n",
        "            return None\n",
        "        \n",
        "        # Sample random starting indices\n",
        "        max_start = len(self.buffer) - seq_length\n",
        "        starts = np.random.randint(0, max_start, size=batch_size)\n",
        "        \n",
        "        obs_seq = []\n",
        "        action_seq = []\n",
        "        reward_seq = []\n",
        "        done_seq = []\n",
        "        \n",
        "        for start in starts:\n",
        "            # Extract sequence\n",
        "            seq = [self.buffer[start + i] for i in range(seq_length)]\n",
        "            \n",
        "            obs_seq.append([s['obs'] for s in seq])\n",
        "            action_seq.append([s['action'] for s in seq])\n",
        "            reward_seq.append([s['reward'] for s in seq])\n",
        "            done_seq.append([s['done'] for s in seq])\n",
        "        \n",
        "        # Convert to tensors\n",
        "        # obs: (batch, seq, 3, 64, 64)\n",
        "        obs_tensor = torch.stack([torch.stack([torch.tensor(o, dtype=torch.float32) for o in obs]) for obs in obs_seq])\n",
        "        # action: (batch, seq, action_dim) - one-hot\n",
        "        action_tensor = torch.stack([torch.stack([F.one_hot(torch.tensor(a, dtype=torch.long), config['action_dim']).float() for a in action]) for action in action_seq])\n",
        "        # reward: (batch, seq)\n",
        "        reward_tensor = torch.stack([torch.tensor(reward, dtype=torch.float32) for reward in reward_seq])\n",
        "        # done: (batch, seq)\n",
        "        done_tensor = torch.stack([torch.tensor(done, dtype=torch.float32) for done in done_seq])\n",
        "        \n",
        "        return obs_tensor, action_tensor, reward_tensor, done_tensor\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_obs(obs):\n",
        "    \"\"\"\n",
        "    Preprocess observation to (3, 64, 64) tensor.\n",
        "    MiniWorld returns (H, W, 3) numpy array.\n",
        "    \"\"\"\n",
        "    if isinstance(obs, np.ndarray):\n",
        "        # Convert to PIL Image if needed\n",
        "        if obs.dtype != np.uint8:\n",
        "            obs = (obs * 255).astype(np.uint8)\n",
        "        img = Image.fromarray(obs)\n",
        "    else:\n",
        "        img = obs\n",
        "    \n",
        "    # Resize to 64x64\n",
        "    img = img.resize((64, 64), Image.LANCZOS)\n",
        "    \n",
        "    # Convert to numpy and normalize to [0, 1]\n",
        "    img_array = np.array(img).astype(np.float32) / 255.0\n",
        "    \n",
        "    # Convert HWC to CHW: (64, 64, 3) -> (3, 64, 64)\n",
        "    img_array = np.transpose(img_array, (2, 0, 1))\n",
        "    \n",
        "    return img_array\n",
        "\n",
        "\n",
        "def heuristic_policy(env, epsilon=0.3):\n",
        "    \"\"\"\n",
        "    Simple heuristic policy: prefers moving forward (action 0), \n",
        "    occasionally takes random actions.\n",
        "    \"\"\"\n",
        "    if np.random.random() < epsilon:\n",
        "        return env.action_space.sample()\n",
        "    else:\n",
        "        return 0  # move_forward\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def collect_trajectory(env, policy_fn, max_steps=500):\n",
        "    \"\"\"\n",
        "    Collect a single trajectory using the exploration policy.\n",
        "    \n",
        "    Returns:\n",
        "        trajectory: list of (obs, action, reward, next_obs, done)\n",
        "    \"\"\"\n",
        "    obs, info = env.reset()\n",
        "    obs = preprocess_obs(obs)\n",
        "    \n",
        "    trajectory = []\n",
        "    total_reward = 0\n",
        "    \n",
        "    for step in range(max_steps):\n",
        "        # Get action from policy\n",
        "        action = policy_fn(env, config['epsilon'])\n",
        "        \n",
        "        # Take step\n",
        "        next_obs, reward, terminated, truncated, info = env.step(action)\n",
        "        next_obs = preprocess_obs(next_obs)\n",
        "        done = terminated or truncated\n",
        "        \n",
        "        # Store transition\n",
        "        trajectory.append({\n",
        "            'obs': obs.copy(),\n",
        "            'action': action,\n",
        "            'reward': float(reward),\n",
        "            'next_obs': next_obs.copy(),\n",
        "            'done': float(done),\n",
        "        })\n",
        "        \n",
        "        total_reward += reward\n",
        "        obs = next_obs\n",
        "        \n",
        "        if done:\n",
        "            break\n",
        "    \n",
        "    return trajectory, total_reward, len(trajectory)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_n_step_returns(rewards, dones, values, gamma=0.99, n_step=5):\n",
        "    \"\"\"\n",
        "    Optimized vectorized n-step returns computation.\n",
        "    G_t = r_t + γ*r_{t+1} + ... + γ^{n-1}*r_{t+n-1} + γ^n * V_{t+n}\n",
        "    \n",
        "    Args:\n",
        "        rewards: (batch, seq) tensor of rewards\n",
        "        dones: (batch, seq) tensor of done flags\n",
        "        values: (batch, seq) tensor of predicted values (for bootstrapping) - should be detached\n",
        "        gamma: discount factor\n",
        "        n_step: number of steps for n-step return\n",
        "    \n",
        "    Returns:\n",
        "        returns: (batch, seq) tensor of n-step returns\n",
        "    \"\"\"\n",
        "    batch_size, seq_length = rewards.shape\n",
        "    device = rewards.device\n",
        "    values = values.detach()\n",
        "    \n",
        "    # Pre-compute discount factors\n",
        "    discounts = gamma ** torch.arange(n_step + 1, device=device, dtype=torch.float32)\n",
        "    \n",
        "    returns = torch.zeros_like(rewards)\n",
        "    \n",
        "    # Vectorized computation per timestep\n",
        "    for t in range(seq_length):\n",
        "        # Get rewards for next n steps\n",
        "        end_idx = min(t + n_step, seq_length)\n",
        "        n_rewards = rewards[:, t:end_idx]  # (B, n_actual)\n",
        "        n_dones = dones[:, t:end_idx]  # (B, n_actual)\n",
        "        \n",
        "        n_actual = n_rewards.shape[1]\n",
        "        disc = discounts[:n_actual].unsqueeze(0)  # (1, n_actual)\n",
        "        \n",
        "        # Mask out rewards after done (cumulative product to stop after first done)\n",
        "        done_mask = torch.cumprod(1.0 - n_dones, dim=1)  # (B, n_actual)\n",
        "        masked_rewards = n_rewards * done_mask\n",
        "        \n",
        "        # Sum discounted rewards\n",
        "        reward_sum = (masked_rewards * disc).sum(dim=1)  # (B,)\n",
        "        \n",
        "        # Bootstrap value\n",
        "        if t + n_step < seq_length:\n",
        "            # Use value at t+n_step if not done\n",
        "            bootstrap = discounts[n_step] * values[:, t + n_step] * (1.0 - dones[:, t + n_step])\n",
        "        else:\n",
        "            # Use last value if sequence ended\n",
        "            bootstrap = discounts[n_actual] * values[:, -1] * (1.0 - dones[:, -1])\n",
        "        \n",
        "        returns[:, t] = reward_sum + bootstrap\n",
        "    \n",
        "    return returns\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Falling back to num_samples=4\n",
            "Falling back to non-multisampled frame buffer\n",
            "Falling back to num_samples=4\n",
            "Falling back to non-multisampled frame buffer\n",
            "Environment: MiniWorld-OneRoom-v0\n",
            "Action space: Discrete(3)\n",
            "Observation space: Box(0, 255, (60, 80, 3), uint8)\n",
            "\n",
            "World Model initialized:\n",
            "  Total parameters: 10,941,945\n",
            "\n",
            "Initialization complete!\n"
          ]
        }
      ],
      "source": [
        "# Initialize environment\n",
        "env = gym.make(config['env_name'], render_mode='rgb_array')\n",
        "print(f\"Environment: {config['env_name']}\")\n",
        "print(f\"Action space: {env.action_space}\")\n",
        "print(f\"Observation space: {env.observation_space}\")\n",
        "\n",
        "# Initialize world model\n",
        "model = WorldModel(\n",
        "    action_dim=config['action_dim'],\n",
        "    embedding_dim=config['embedding_dim'],\n",
        "    hidden_dim=config['hidden_dim'],\n",
        "    stochastic_dim=config['stochastic_dim'],\n",
        "    action_space_size=config['action_dim'],\n",
        ").to(device)\n",
        "\n",
        "print(f\"\\nWorld Model initialized:\")\n",
        "print(f\"  Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "\n",
        "# Initialize optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'])\n",
        "\n",
        "# Initialize replay buffer\n",
        "replay_buffer = ReplayBuffer(capacity=10000)\n",
        "\n",
        "print(\"\\nInitialization complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.1 Data Collection Phase\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting initial trajectories...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Collecting: 100%|██████████| 100/100 [00:08<00:00, 11.18it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Collected 10000 transitions\n",
            "Average reward: 0.07 ± 0.24\n",
            "Average length: 171.25 ± 34.45\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Initial data collection\n",
        "print(\"Collecting initial trajectories...\")\n",
        "episode_rewards = []\n",
        "episode_lengths = []\n",
        "\n",
        "for episode in tqdm(range(config['num_collection_episodes']), desc=\"Collecting\"):\n",
        "    trajectory, total_reward, traj_length = collect_trajectory(\n",
        "        env, heuristic_policy, max_steps=500\n",
        "    )\n",
        "    replay_buffer.add_trajectory(trajectory)\n",
        "    episode_rewards.append(total_reward)\n",
        "    episode_lengths.append(traj_length)\n",
        "\n",
        "print(f\"\\nCollected {len(replay_buffer)} transitions\")\n",
        "print(f\"Average reward: {np.mean(episode_rewards):.2f} ± {np.std(episode_rewards):.2f}\")\n",
        "print(f\"Average length: {np.mean(episode_lengths):.2f} ± {np.std(episode_lengths):.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.2 Training Loop\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting training with KL annealing and free bits...\n",
            "KL weight: 0.000 -> 0.500 over 2000 steps\n",
            "Free nats: 3.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:   1%|          | 100/10000 [00:17<27:20,  6.04it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Step 100/10000\n",
            "  Total loss: 0.1574\n",
            "  Recon: 0.0083, Reward: 0.0001\n",
            "  KL (raw/clamped): 0.0242/3.0000, KL weight: 0.0248\n",
            "  Value: 0.0001\n",
            "  Post/Prior std: 0.9567/1.0013\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:   2%|▏         | 200/10000 [00:34<27:22,  5.97it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Step 200/10000\n",
            "  Total loss: 0.2202\n",
            "  Recon: 0.0071, Reward: 0.0000\n",
            "  KL (raw/clamped): 0.4367/3.0000, KL weight: 0.0498\n",
            "  Value: 0.0001\n",
            "  Post/Prior std: 0.7474/1.0054\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:   3%|▎         | 300/10000 [00:52<28:49,  5.61it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Step 300/10000\n",
            "  Total loss: 0.2882\n",
            "  Recon: 0.0064, Reward: 0.0000\n",
            "  KL (raw/clamped): 1.2786/3.0000, KL weight: 0.0747\n",
            "  Value: 0.0002\n",
            "  Post/Prior std: 0.3154/1.0255\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:   4%|▍         | 400/10000 [01:10<28:26,  5.62it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Step 400/10000\n",
            "  Total loss: 0.3671\n",
            "  Recon: 0.0068, Reward: 0.0000\n",
            "  KL (raw/clamped): 1.6696/3.0001, KL weight: 0.0998\n",
            "  Value: 0.0001\n",
            "  Post/Prior std: 0.1639/1.0651\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:   5%|▌         | 500/10000 [01:28<29:01,  5.45it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Step 500/10000\n",
            "  Total loss: 0.4409\n",
            "  Recon: 0.0067, Reward: 0.0000\n",
            "  KL (raw/clamped): 1.8894/3.0000, KL weight: 0.1247\n",
            "  Value: 0.0001\n",
            "  Post/Prior std: 0.1264/1.0856\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:   6%|▌         | 600/10000 [01:47<30:56,  5.06it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Step 600/10000\n",
            "  Total loss: 0.5037\n",
            "  Recon: 0.0054, Reward: 0.0000\n",
            "  KL (raw/clamped): 2.1045/3.0000, KL weight: 0.1497\n",
            "  Value: 0.0000\n",
            "  Post/Prior std: 0.1022/1.0815\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:   7%|▋         | 674/10000 [02:03<28:29,  5.46it/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 103\u001b[39m\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(T):\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m all_outputs[t][\u001b[33m'\u001b[39m\u001b[33mprior_dist\u001b[39m\u001b[33m'\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m all_outputs[t][\u001b[33m'\u001b[39m\u001b[33mpost_dist\u001b[39m\u001b[33m'\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    102\u001b[39m         \u001b[38;5;66;03m# Per-dimension KL divergence\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m         kl_per_dim = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdistributions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mkl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mkl_divergence\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    104\u001b[39m \u001b[43m            \u001b[49m\u001b[43mall_outputs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpost_dist\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    105\u001b[39m \u001b[43m            \u001b[49m\u001b[43mall_outputs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mprior_dist\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (B, stochastic_dim)\u001b[39;00m\n\u001b[32m    108\u001b[39m         \u001b[38;5;66;03m# Raw KL (for logging)\u001b[39;00m\n\u001b[32m    109\u001b[39m         kl_t_raw = kl_per_dim.mean()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/worldmodels/lib/python3.14/site-packages/torch/distributions/kl.py:192\u001b[39m, in \u001b[36mkl_divergence\u001b[39m\u001b[34m(p, q)\u001b[39m\n\u001b[32m    188\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m fun \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[32m    189\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[32m    190\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNo KL(p || q) is implemented for p type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m and q type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mq.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    191\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m192\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/worldmodels/lib/python3.14/site-packages/torch/distributions/kl.py:469\u001b[39m, in \u001b[36m_kl_normal_normal\u001b[39m\u001b[34m(p, q)\u001b[39m\n\u001b[32m    466\u001b[39m \u001b[38;5;129m@register_kl\u001b[39m(Normal, Normal)\n\u001b[32m    467\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_kl_normal_normal\u001b[39m(p, q):\n\u001b[32m    468\u001b[39m     var_ratio = (p.scale / q.scale).pow(\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m469\u001b[39m     t1 = \u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloc\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mq\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[43mq\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscale\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpow\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    470\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[32m0.5\u001b[39m * (var_ratio + t1 - \u001b[32m1\u001b[39m - var_ratio.log())\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "# Training loop with KL annealing and free bits\n",
        "model.train()\n",
        "losses_history = {\n",
        "    'total': [],\n",
        "    'recon': [],\n",
        "    'reward': [],\n",
        "    'kl': [],\n",
        "    'kl_raw': [],  # Track raw KL before free bits\n",
        "    'value': [],\n",
        "    'kl_weight': [],  # Track annealing schedule\n",
        "    'posterior_std': [],  # Track posterior collapse\n",
        "    'prior_std': [],\n",
        "}\n",
        "\n",
        "print(\"Starting training with KL annealing and free bits...\")\n",
        "print(f\"KL weight: {config['lambda_kl_start']:.3f} -> {config['lambda_kl_end']:.3f} over {config['kl_anneal_steps']} steps\")\n",
        "print(f\"Free nats: {config['free_nats']:.1f}\")\n",
        "\n",
        "for step in tqdm(range(config['num_training_steps']), desc=\"Training\"):\n",
        "    # Collect new data periodically\n",
        "    if step % config['collect_every_n_steps'] == 0 and step > 0:\n",
        "        trajectory, _, _ = collect_trajectory(env, heuristic_policy, max_steps=500)\n",
        "        replay_buffer.add_trajectory(trajectory)\n",
        "    \n",
        "    # KL annealing schedule\n",
        "    if step < config['kl_anneal_steps']:\n",
        "        kl_weight = config['lambda_kl_start'] + (config['lambda_kl_end'] - config['lambda_kl_start']) * (step / config['kl_anneal_steps'])\n",
        "    else:\n",
        "        kl_weight = config['lambda_kl_end']\n",
        "    \n",
        "    # Sample batch of sequences\n",
        "    batch = replay_buffer.sample_sequences(\n",
        "        batch_size=config['batch_size'],\n",
        "        seq_length=config['seq_length']\n",
        "    )\n",
        "    \n",
        "    if batch is None:\n",
        "        continue\n",
        "    \n",
        "    obs_seq, action_seq, reward_seq, done_seq = batch\n",
        "    obs_seq = obs_seq.to(device)  # (B, T, 3, 64, 64)\n",
        "    action_seq = action_seq.to(device)  # (B, T, action_dim)\n",
        "    reward_seq = reward_seq.to(device)  # (B, T)\n",
        "    done_seq = done_seq.to(device)  # (B, T)\n",
        "    \n",
        "    # Reshape for processing: (B*T, ...)\n",
        "    B, T = obs_seq.shape[:2]\n",
        "    obs_flat = obs_seq.view(B * T, 3, 64, 64)\n",
        "    action_flat = action_seq.view(B * T, config['action_dim'])\n",
        "    reward_flat = reward_seq.view(B * T)\n",
        "    \n",
        "    # Initialize states\n",
        "    h_prev = None\n",
        "    z_prev = None\n",
        "    \n",
        "    # Forward pass through sequence\n",
        "    all_outputs = []\n",
        "    for t in range(T):\n",
        "        obs_t = obs_seq[:, t]  # (B, 3, 64, 64)\n",
        "        action_t = action_seq[:, t]  # (B, action_dim)\n",
        "        \n",
        "        # Use previous action for RSSM (or zero for first step)\n",
        "        if t == 0:\n",
        "            action_prev = torch.zeros(B, config['action_dim'], device=device)\n",
        "        else:\n",
        "            action_prev = action_seq[:, t-1]\n",
        "        \n",
        "        # Forward pass\n",
        "        outputs = model(obs_t, action_prev, h_prev, z_prev, use_posterior=True)\n",
        "        all_outputs.append(outputs)\n",
        "        \n",
        "        # Update states for next step\n",
        "        h_prev = outputs['h_t']\n",
        "        z_prev = outputs['z_t']\n",
        "    \n",
        "    # Stack outputs: (B, T, ...)\n",
        "    o_hat_seq = torch.stack([o['o_hat_t'] for o in all_outputs], dim=1)  # (B, T, 3, 64, 64)\n",
        "    r_hat_seq = torch.stack([o['r_hat_t'] for o in all_outputs], dim=1)  # (B, T)\n",
        "    v_hat_seq = torch.stack([o['v_hat_t'] for o in all_outputs], dim=1)  # (B, T)\n",
        "    \n",
        "    # Compute n-step returns for value targets (detach values for bootstrapping)\n",
        "    with torch.no_grad():\n",
        "        value_targets = compute_n_step_returns(\n",
        "            reward_seq, done_seq, v_hat_seq.detach(),\n",
        "            gamma=config['gamma'],\n",
        "            n_step=config['n_step']\n",
        "        )\n",
        "    \n",
        "    # Compute losses\n",
        "    recon_loss = F.mse_loss(o_hat_seq, obs_seq)\n",
        "    reward_loss = F.mse_loss(r_hat_seq, reward_seq)\n",
        "    value_loss = F.mse_loss(v_hat_seq, value_targets)\n",
        "    \n",
        "    # KL loss with FREE BITS constraint (sum over sequence, mean over batch)\n",
        "    kl_loss_raw = torch.tensor(0.0, device=device)\n",
        "    kl_loss = torch.tensor(0.0, device=device)\n",
        "    posterior_stds = []\n",
        "    prior_stds = []\n",
        "    \n",
        "    for t in range(T):\n",
        "        if all_outputs[t]['prior_dist'] is not None and all_outputs[t]['post_dist'] is not None:\n",
        "            # Per-dimension KL divergence\n",
        "            kl_per_dim = torch.distributions.kl.kl_divergence(\n",
        "                all_outputs[t]['post_dist'],\n",
        "                all_outputs[t]['prior_dist']\n",
        "            )  # (B, stochastic_dim)\n",
        "            \n",
        "            # Raw KL (for logging)\n",
        "            kl_t_raw = kl_per_dim.mean()\n",
        "            kl_loss_raw += kl_t_raw\n",
        "            \n",
        "            # Free bits: max(kl_per_dim, free_nats)\n",
        "            # This prevents the KL from going below free_nats per dimension\n",
        "            kl_per_dim_clamped = torch.maximum(kl_per_dim, torch.tensor(config['free_nats'], device=device))\n",
        "            kl_t = kl_per_dim_clamped.mean()\n",
        "            kl_loss += kl_t\n",
        "            \n",
        "            # Track std for diagnosing posterior collapse\n",
        "            posterior_stds.append(all_outputs[t]['post_dist'].stddev.mean().item())\n",
        "            prior_stds.append(all_outputs[t]['prior_dist'].stddev.mean().item())\n",
        "    \n",
        "    kl_loss = kl_loss / T\n",
        "    kl_loss_raw = kl_loss_raw / T\n",
        "    \n",
        "    # Total loss\n",
        "    total_loss = (\n",
        "        config['lambda_rec'] * recon_loss +\n",
        "        kl_weight * kl_loss +\n",
        "        config['lambda_reward'] * reward_loss +\n",
        "        config['lambda_value'] * value_loss\n",
        "    )\n",
        "    \n",
        "    # Backward pass\n",
        "    optimizer.zero_grad()\n",
        "    total_loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "    optimizer.step()\n",
        "    \n",
        "    # Log losses\n",
        "    losses_history['total'].append(total_loss.item())\n",
        "    losses_history['recon'].append(recon_loss.item())\n",
        "    losses_history['reward'].append(reward_loss.item())\n",
        "    losses_history['kl'].append(kl_loss.item())\n",
        "    losses_history['kl_raw'].append(kl_loss_raw.item())\n",
        "    losses_history['value'].append(value_loss.item())\n",
        "    losses_history['kl_weight'].append(kl_weight)\n",
        "    losses_history['posterior_std'].append(np.mean(posterior_stds) if posterior_stds else 0)\n",
        "    losses_history['prior_std'].append(np.mean(prior_stds) if prior_stds else 0)\n",
        "    \n",
        "    # Print progress (less frequently to reduce overhead)\n",
        "    if (step + 1) % 100 == 0:\n",
        "        print(f\"\\nStep {step + 1}/{config['num_training_steps']}\")\n",
        "        print(f\"  Total loss: {total_loss.item():.4f}\")\n",
        "        print(f\"  Recon: {recon_loss.item():.4f}, Reward: {reward_loss.item():.4f}\")\n",
        "        print(f\"  KL (raw/clamped): {kl_loss_raw.item():.4f}/{kl_loss.item():.4f}, KL weight: {kl_weight:.4f}\")\n",
        "        print(f\"  Value: {value_loss.item():.4f}\")\n",
        "        print(f\"  Post/Prior std: {np.mean(posterior_stds):.4f}/{np.mean(prior_stds):.4f}\")\n",
        "\n",
        "print(\"\\nTraining complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot training losses with enhanced diagnostics\n",
        "fig, axes = plt.subplots(3, 3, figsize=(18, 12))\n",
        "\n",
        "# Row 1: Main losses\n",
        "axes[0, 0].plot(losses_history['total'])\n",
        "axes[0, 0].set_title('Total Loss')\n",
        "axes[0, 0].set_xlabel('Step')\n",
        "axes[0, 0].set_ylabel('Loss')\n",
        "axes[0, 0].grid(True)\n",
        "\n",
        "axes[0, 1].plot(losses_history['recon'])\n",
        "axes[0, 1].set_title('Reconstruction Loss')\n",
        "axes[0, 1].set_xlabel('Step')\n",
        "axes[0, 1].set_ylabel('Loss')\n",
        "axes[0, 1].grid(True)\n",
        "\n",
        "axes[0, 2].plot(losses_history['reward'])\n",
        "axes[0, 2].set_title('Reward Prediction Loss')\n",
        "axes[0, 2].set_xlabel('Step')\n",
        "axes[0, 2].set_ylabel('Loss')\n",
        "axes[0, 2].grid(True)\n",
        "\n",
        "# Row 2: KL diagnostics\n",
        "axes[1, 0].plot(losses_history['kl_raw'], label='Raw KL', alpha=0.7)\n",
        "axes[1, 0].plot(losses_history['kl'], label='Clamped KL (free bits)', alpha=0.7)\n",
        "axes[1, 0].set_title('KL Divergence Loss')\n",
        "axes[1, 0].set_xlabel('Step')\n",
        "axes[1, 0].set_ylabel('Loss')\n",
        "axes[1, 0].legend()\n",
        "axes[1, 0].grid(True)\n",
        "\n",
        "axes[1, 1].plot(losses_history['kl_weight'])\n",
        "axes[1, 1].set_title('KL Weight (Annealing Schedule)')\n",
        "axes[1, 1].set_xlabel('Step')\n",
        "axes[1, 1].set_ylabel('Weight')\n",
        "axes[1, 1].grid(True)\n",
        "\n",
        "axes[1, 2].plot(losses_history['value'])\n",
        "axes[1, 2].set_title('Value Prediction Loss')\n",
        "axes[1, 2].set_xlabel('Step')\n",
        "axes[1, 2].set_ylabel('Loss')\n",
        "axes[1, 2].grid(True)\n",
        "\n",
        "# Row 3: Posterior collapse diagnostics\n",
        "axes[2, 0].plot(losses_history['posterior_std'], label='Posterior', alpha=0.7)\n",
        "axes[2, 0].plot(losses_history['prior_std'], label='Prior', alpha=0.7)\n",
        "axes[2, 0].set_title('Latent Distribution Std Devs')\n",
        "axes[2, 0].set_xlabel('Step')\n",
        "axes[2, 0].set_ylabel('Std Dev')\n",
        "axes[2, 0].legend()\n",
        "axes[2, 0].grid(True)\n",
        "axes[2, 0].axhline(y=0.1, color='r', linestyle='--', alpha=0.3, label='Collapse threshold')\n",
        "\n",
        "# Histogram of final losses\n",
        "axes[2, 1].hist(losses_history['recon'][-1000:], bins=30, alpha=0.7)\n",
        "axes[2, 1].set_title('Recent Reconstruction Loss Distribution')\n",
        "axes[2, 1].set_xlabel('Loss')\n",
        "axes[2, 1].set_ylabel('Frequency')\n",
        "axes[2, 1].grid(True)\n",
        "\n",
        "# Histogram of KL\n",
        "axes[2, 2].hist(losses_history['kl_raw'][-1000:], bins=30, alpha=0.7, label='Raw')\n",
        "axes[2, 2].hist(losses_history['kl'][-1000:], bins=30, alpha=0.5, label='Clamped')\n",
        "axes[2, 2].set_title('Recent KL Loss Distribution')\n",
        "axes[2, 2].set_xlabel('Loss')\n",
        "axes[2, 2].set_ylabel('Frequency')\n",
        "axes[2, 2].legend()\n",
        "axes[2, 2].grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print summary statistics\n",
        "print(\"\\n=== Training Summary ===\")\n",
        "print(f\"Final reconstruction loss: {losses_history['recon'][-1]:.6f}\")\n",
        "print(f\"Final KL (raw/clamped): {losses_history['kl_raw'][-1]:.6f} / {losses_history['kl'][-1]:.6f}\")\n",
        "print(f\"Final posterior std: {losses_history['posterior_std'][-1]:.6f}\")\n",
        "print(f\"Final prior std: {losses_history['prior_std'][-1]:.6f}\")\n",
        "print(f\"\\nPosterior collapse check:\")\n",
        "if losses_history['posterior_std'][-1] < 0.1:\n",
        "    print(\"  ⚠️ WARNING: Posterior may have collapsed (std < 0.1)\")\n",
        "elif losses_history['posterior_std'][-1] > 0.5:\n",
        "    print(\"  ✅ GOOD: Posterior is active (std > 0.5)\")\n",
        "else:\n",
        "    print(\"  ⚠️ MARGINAL: Posterior std is low but not collapsed\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize reconstructions with detailed statistics\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    # Sample a sequence\n",
        "    batch = replay_buffer.sample_sequences(batch_size=1, seq_length=10)\n",
        "    if batch is not None:\n",
        "        obs_seq, action_seq, reward_seq, done_seq = batch\n",
        "        obs_seq = obs_seq.to(device)\n",
        "        action_seq = action_seq.to(device)\n",
        "        \n",
        "        # Reconstruct\n",
        "        h_prev = None\n",
        "        z_prev = None\n",
        "        reconstructions = []\n",
        "        \n",
        "        for t in range(min(10, obs_seq.shape[1])):\n",
        "            obs_t = obs_seq[:, t]\n",
        "            if t == 0:\n",
        "                action_prev = torch.zeros(1, config['action_dim'], device=device)\n",
        "            else:\n",
        "                action_prev = action_seq[:, t-1]\n",
        "            \n",
        "            outputs = model(obs_t, action_prev, h_prev, z_prev, use_posterior=True)\n",
        "            reconstructions.append(outputs['o_hat_t'])\n",
        "            h_prev = outputs['h_t']\n",
        "            z_prev = outputs['z_t']\n",
        "        \n",
        "        # Plot original vs reconstructed\n",
        "        fig, axes = plt.subplots(2, 10, figsize=(20, 4))\n",
        "        for t in range(min(10, len(reconstructions))):\n",
        "            # Original\n",
        "            orig = obs_seq[0, t].cpu().numpy().transpose(1, 2, 0)\n",
        "            axes[0, t].imshow(np.clip(orig, 0, 1))\n",
        "            axes[0, t].set_title(f'Original t={t}')\n",
        "            axes[0, t].axis('off')\n",
        "            \n",
        "            # Reconstructed\n",
        "            recon = reconstructions[t][0].cpu().numpy().transpose(1, 2, 0)\n",
        "            axes[1, t].imshow(np.clip(recon, 0, 1))\n",
        "            axes[1, t].set_title(f'Reconstructed t={t}')\n",
        "            axes[1, t].axis('off')\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "        # Print detailed statistics for the first frame\n",
        "        print(\"\\n=== Reconstruction Statistics (t=0) ===\")\n",
        "        orig_0 = obs_seq[0, 0].cpu().numpy()\n",
        "        recon_0 = reconstructions[0][0].cpu().numpy()\n",
        "        \n",
        "        print(f\"Original image:\")\n",
        "        print(f\"  Mean: {orig_0.mean():.4f}, Std: {orig_0.std():.4f}\")\n",
        "        print(f\"  Min: {orig_0.min():.4f}, Max: {orig_0.max():.4f}\")\n",
        "        \n",
        "        print(f\"\\nReconstructed image:\")\n",
        "        print(f\"  Mean: {recon_0.mean():.4f}, Std: {recon_0.std():.4f}\")\n",
        "        print(f\"  Min: {recon_0.min():.4f}, Max: {recon_0.max():.4f}\")\n",
        "        \n",
        "        # Per-channel statistics\n",
        "        print(f\"\\nPer-channel (RGB) statistics:\")\n",
        "        for c, color in enumerate(['Red', 'Green', 'Blue']):\n",
        "            print(f\"  {color} - Orig: {orig_0[c].mean():.4f}, Recon: {recon_0[c].mean():.4f}\")\n",
        "        \n",
        "        # MSE\n",
        "        mse = np.mean((orig_0 - recon_0) ** 2)\n",
        "        print(f\"\\nMSE: {mse:.6f}\")\n",
        "        print(f\"RMSE: {np.sqrt(mse):.6f}\")\n",
        "        \n",
        "        # Check if reconstruction is constant\n",
        "        if recon_0.std() < 0.01:\n",
        "            print(\"\\n⚠️ WARNING: Reconstruction has very low variance - likely outputting constant values!\")\n",
        "            print(f\"This suggests the decoder is not learning. Check:\")\n",
        "            print(f\"  1. Is the reconstruction loss actually backpropagating?\")\n",
        "            print(f\"  2. Is the KL loss too strong initially?\")\n",
        "            print(f\"  3. Are the latent codes carrying information?\")\n",
        "        else:\n",
        "            print(f\"\\n✅ Reconstruction has variance: {recon_0.std():.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.save({\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "    'config': config,\n",
        "    'losses_history': losses_history,\n",
        "}, 'worldmodel_checkpoint.pth')\n",
        "print(\"Model saved to worldmodel_checkpoint.pth\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "worldmodels",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
